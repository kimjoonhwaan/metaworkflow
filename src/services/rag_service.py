"""RAG (Retrieval-Augmented Generation) service with metadata-based search"""

import asyncio
import time
from typing import List, Dict, Any, Optional
import json
import re
from datetime import datetime

import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
import tiktoken
import numpy as np

from sqlalchemy.orm import Session, joinedload
from sqlalchemy import and_, or_

from ..database.models import (
    KnowledgeBase, Document, DocumentMetadata, DocumentChunk, RAGQuery,
    KnowledgeBaseCategory, DocumentContentType, Domain
)
from ..database.session import get_session
from ..utils.config import get_settings
from ..utils.logger import get_logger
from ..utils.openai_client import get_openai_client
from .domain_service import get_domain_service

logger = get_logger(__name__)


class RAGService:
    """
    Metadata-based RAG service with domain-based collection separation
    - Embeds only metadata (title, keywords, technologies, description)
    - Stores full content in database
    - Retrieves full content after metadata match
    - Separates documents by domain (naver, weather, kakao, google, common)
    """
    
    # Note: Domain management is now handled dynamically via DomainService
    # No hardcoded domain lists needed!
    
    def __init__(self):
        self.settings = get_settings()
        self.openai_client = get_openai_client()
        
        # âœ¨ NEW: Domain service for dynamic domain management
        self.domain_service = get_domain_service()
        
        # Disable ChromaDB telemetry and logging
        import os
        import logging
        os.environ["ANONYMIZED_TELEMETRY"] = "False"
        os.environ["CHROMA_TELEMETRY_ENABLED"] = "False"
        
        # Suppress ChromaDB's telemetry logger completely
        logging.getLogger("chromadb.telemetry").setLevel(logging.CRITICAL)
        logging.getLogger("chromadb").setLevel(logging.ERROR)
        logging.getLogger("chromadb.telemetry.posthog").setLevel(logging.CRITICAL)
        
        # Initialize ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path="./data/chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # âœ¨ OpenAI embedding function for ChromaDB
        self.embedding_function = OpenAIEmbeddingFunction(
            api_key=self.settings.openai_api_key,
            model_name="text-embedding-3-small"
        )
        
        # Tokenizer for text processing
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # Collection cache
        self._collections_cache = {}
    
    def _get_collection_name_for_domain(self, domain: str) -> str:
        """
        Get ChromaDB collection name for domain (dynamic)
        
        Args:
            domain: Domain name (from database)
        
        Returns:
            Collection name in ChromaDB (e.g., "collection_ë„¤ì´ë²„")
        """
        # Get domain from database to get collection name
        domain_obj = self.domain_service.get_domain_by_name(domain)
        
        if domain_obj:
            collection_name = domain_obj.collection_name
        else:
            # Fallback: generate collection name
            collection_name = f"collection_{domain}"
            logger.warning(f"âš ï¸ Domain '{domain}' not found in database, using fallback: {collection_name}")
        
        logger.debug(f"ðŸ“‚ Collection for domain '{domain}': {collection_name}")
        return collection_name
    
    def _get_collection_for_domain(self, domain: str):
        """
        Get or create ChromaDB collection for domain
        
        Args:
            domain: Domain name
        
        Returns:
            ChromaDB Collection object
        """
        collection_name = self._get_collection_name_for_domain(domain)
        cache_key = f"domain_{domain}"
        
        if cache_key not in self._collections_cache:
            try:
                # Try to get existing collection
                collection = self.chroma_client.get_collection(
                    name=collection_name,
                    embedding_function=self.embedding_function
                )
                logger.debug(f"âœ… Got existing collection: {collection_name}")
            except Exception:
                # Create new collection if doesn't exist
                collection = self.chroma_client.create_collection(
                    name=collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"hnsw:space": "cosine"}
                )
                logger.info(f"âœ¨ Created new collection: {collection_name}")
            
            self._collections_cache[cache_key] = collection
        
        return self._collections_cache[cache_key]
    
    def _get_collection_name(self, category: KnowledgeBaseCategory) -> str:
        """Get ChromaDB collection name for category"""
        return f"metadata_{category.value.lower()}"
    
    def _get_or_create_collection(self, category: KnowledgeBaseCategory):
        """Get or create ChromaDB collection with OpenAI embeddings"""
        collection_name = self._get_collection_name(category)
        
        if collection_name not in self._collections_cache:
            try:
                # Try to get existing collection first
                try:
                    collection = self.chroma_client.get_collection(collection_name)
                    logger.info(f"âœ… Using existing collection: {collection_name}")
                except Exception as e:
                    # Collection doesn't exist, create new one
                    logger.info(f"âœ¨ Creating new collection: {collection_name}")
                    collection = self.chroma_client.create_collection(
                        name=collection_name,
                        metadata={"category": category.value},
                        embedding_function=self.embedding_function
                    )
                    
            except Exception as e:
                logger.error(f"Failed to get or create collection {collection_name}: {e}")
                raise
            
            self._collections_cache[collection_name] = collection
        
        return self._collections_cache[collection_name]
    
    async def add_document(
        self,
        document: Document,
        metadata_obj: DocumentMetadata,
        domain: str = None  # âœ¨ NEW: Optional domain parameter
    ) -> bool:
        """
        Add document to RAG system with domain-based collection
        - Embeds metadata (searchable_text) to domain-specific collection
        - References full content in database
        
        Args:
            document: Document object with content
            metadata_obj: DocumentMetadata object
            domain: Optional domain override (default: uses document.domain)
        """
        try:
            # âœ¨ Step 1: Get domain (priority: parameter > document.domain > "common")
            doc_domain = domain or document.domain or "common"
            logger.info(f"ðŸ“ Adding document to domain '{doc_domain}': {document.title}")
            
            # âœ¨ Step 2: Update document domain if specified
            if domain and document.domain != domain:
                document.domain = domain
                logger.debug(f"ðŸ“ Updated document domain to '{domain}'")
            
            # âœ¨ Step 3: Get domain-specific collection
            collection = self._get_collection_for_domain(doc_domain)
            
            # âœ¨ Step 4: Prepare metadata for ChromaDB
            chroma_metadata = {
                "document_id": document.id,
                "title": document.title,
                "domain": doc_domain,  # âœ¨ Store domain for filtering
                "doc_type": metadata_obj.doc_type or "unknown",
                "content_type": document.content_type.value,
            }
            
            # âœ¨ Step 5: Prepare embedding text
            keywords_str = " ".join(metadata_obj.keywords or []) if metadata_obj.keywords else ""
            searchable_with_title = (
                f"{document.title}\n"
                f"{keywords_str}\n"
                f"{metadata_obj.searchable_text}"
            ).strip()
            
            # âœ¨ Step 6: Add to domain-specific collection
            collection.add(
                ids=[document.id],
                documents=[searchable_with_title],
                metadatas=[chroma_metadata]
            )
            
            # Log what's being embedded
            logger.info(f"ðŸ“ Embedding text (first 150 chars): {searchable_with_title[:150]}...")
            
            # Log metadata for verification
            logger.info(f"ðŸ“Œ Metadata stored in ChromaDB:")
            logger.info(f"   - Title: {chroma_metadata['title']}")
            logger.info(f"   - Domain: {chroma_metadata['domain']}")
            logger.info(f"   - Doc Type: {chroma_metadata['doc_type']}")
            logger.info(f"   - Document ID: {chroma_metadata['document_id']}")
            
            # âœ¨ Step 7: Update embedding_id and domain in database
            with get_session() as session:
                metadata = session.query(DocumentMetadata).filter(
                    DocumentMetadata.document_id == document.id
                ).first()
                if metadata:
                    metadata.embedding_id = document.id
                    metadata.domain = doc_domain  # âœ¨ Store domain
                    session.commit()
                    logger.debug(f"âœ… Updated metadata for document {document.id}")
            
            logger.info(f"âœ… Added document to {doc_domain} collection: {document.title} (ID: {document.id})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to add document {document.id}: {e}")
            raise
    
    async def search_metadata(
        self,
        query: str,
        domain: str = None,  # âœ¨ NEW: Domain parameter for targeted search
        category: KnowledgeBaseCategory = None,  # For backward compatibility
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Search for documents by metadata with domain-based collection separation
        
        Args:
            query: Search query
            domain: Domain name (None = search all domains)
            category: Legacy parameter (ignored in domain-based search)
            limit: Number of results to return
        
        Returns:
            List of search results with metadata and content
        """
        try:
            logger.info(f"ðŸ” Searching: '{query}' in domain: {domain or 'all'}")
            
            if domain:
                # âœ¨ Step 1: Specific domain + common domain search
                all_results = []
                
                # 1-1. Search specific domain collection
                try:
                    specific_collection = self._get_collection_for_domain(domain)
                    specific_results = specific_collection.query(
                        query_texts=[query],
                        n_results=limit,
                        include=["documents", "metadatas", "distances"]
                    )
                    specific_items = self._parse_search_results(specific_results)
                    all_results.extend(specific_items)
                    logger.debug(f"  ðŸ“‚ {domain}: {len(specific_items)} results")
                except Exception as e:
                    logger.debug(f"  âš ï¸ {domain} search failed: {e}")
                
                # 1-2. Search common collection
                try:
                    common_collection = self._get_collection_for_domain("common")
                    common_results = common_collection.query(
                        query_texts=[query],
                        n_results=limit,
                        include=["documents", "metadatas", "distances"]
                    )
                    common_items = self._parse_search_results(common_results)
                    
                    # Remove duplicates (same document_id)
                    existing_ids = {r["document_id"] for r in all_results}
                    unique_common = [
                        r for r in common_items
                        if r["document_id"] not in existing_ids
                    ]
                    
                    all_results.extend(unique_common)
                    logger.debug(f"  ðŸ“‚ common: {len(unique_common)} unique results")
                
                except Exception as e:
                    logger.debug(f"  âš ï¸ common collection search failed: {e}")
                
                # 1-3. Sort by domain (specific first) and then by similarity
                all_results.sort(key=lambda x: (
                    x["domain"] != domain,  # Specific domain first
                    -x["similarity_score"]  # Then by similarity
                ))
                
                final_results = all_results[:limit]
                logger.info(f"âœ… Found {len(final_results)} results in '{domain}' + common")
            
            else:
                # âœ¨ Step 2: Search all domains
                all_results = []
                
                # Get all active domains dynamically
                all_domains = self.domain_service.get_all_domains()
                
                for domain_obj in all_domains:
                    domain_key = domain_obj.name
                    try:
                        collection = self._get_collection_for_domain(domain_key)
                        
                        results = collection.query(
                            query_texts=[query],
                            n_results=limit,
                            include=["documents", "metadatas", "distances"]
                        )
                        
                        domain_results = self._parse_search_results(results)
                        all_results.extend(domain_results)
                        logger.debug(f"  ðŸ“‚ {domain_key}: {len(domain_results)} results")
                    
                    except Exception as e:
                        logger.debug(f"  âš ï¸ {domain_key} search failed: {e}")
                        continue
                
                # Sort by similarity
                all_results.sort(key=lambda x: x["similarity_score"], reverse=True)
                final_results = all_results[:limit]
                
                logger.info(f"âœ… Found {len(final_results)} total results from all domains")
            
            return final_results
        
        except Exception as e:
            logger.error(f"âŒ Search failed: {e}")
            return []
    
    def _parse_search_results(self, results) -> List[Dict]:
        """Parse ChromaDB search results into standardized format"""
        all_results = []
        
        if results and results["ids"] and len(results["ids"]) > 0:
            for i, doc_id in enumerate(results["ids"][0]):
                metadata = results["metadatas"][0][i]
                distance = results["distances"][0][i]
                
                # Calculate similarity score
                if distance < 0.1:
                    similarity = 1.0 - (distance * 2)
                else:
                    similarity = max(0, 1.0 - (distance / 2.0))
                similarity = max(0, min(1, similarity))
                
                result_item = {
                    "document_id": doc_id,
                    "title": metadata.get("title", "Unknown"),
                    "domain": metadata.get("domain", "unknown"),  # âœ¨ Include domain
                    "doc_type": metadata.get("doc_type", "unknown"),
                    "similarity_score": similarity,
                    "distance": distance,
                    "content": ""
                }
                
                # Retrieve full content from database
                try:
                    with get_session() as session:
                        doc = session.query(Document).filter(Document.id == doc_id).first()
                        if doc:
                            result_item["content"] = doc.content[:500] if doc.content else ""
                except Exception as e:
                    logger.debug(f"âš ï¸ Failed to retrieve content for {doc_id}: {e}")
                
                all_results.append(result_item)
        
        return all_results
    
    async def _get_query_embedding(self, query: str) -> Optional[List[float]]:
        """Get embedding for query text"""
        try:
            response = await self.openai_client.embeddings.create(
                input=query,
                model="text-embedding-3-small"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to create query embedding: {e}")
            return None
    
    async def get_full_content(
        self,
        document_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Get full content of documents from database
        Called after metadata search
        """
        try:
            results = []
            
            with get_session() as session:
                for doc_id in document_ids:
                    doc = session.query(Document).filter(
                        Document.id == doc_id
                    ).first()
                    
                    if doc:
                        results.append({
                            "document_id": doc.id,
                            "title": doc.title,
                            "content": doc.content,  # âœ¨ Full content
                            "content_type": doc.content_type.value,
                            "tags": doc.tags or [],
                            "metadata": doc.kb_metadata or {}
                        })
                    else:
                        logger.warning(f"Document not found: {doc_id}")
            
            return results
        
        except Exception as e:
            logger.error(f"Failed to get full content: {e}")
            return []
    
    async def get_document_detail(
        self,
        document_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a single document
        Includes full content and all metadata
        
        Args:
            document_id: Document ID to retrieve
        
        Returns:
            Dictionary with complete document information or None if not found
        """
        try:
            with get_session() as session:
                # Query document with related objects
                doc = session.query(Document).filter(
                    Document.id == document_id
                ).first()
                
                if not doc:
                    logger.warning(f"Document not found: {document_id}")
                    return None
                
                # Get metadata
                metadata = session.query(DocumentMetadata).filter(
                    DocumentMetadata.document_id == document_id
                ).first()
                
                # Get chunks if available
                chunks = session.query(DocumentChunk).filter(
                    DocumentChunk.document_id == document_id
                ).all()
                
                # Build response
                result = {
                    "document_id": doc.id,
                    "title": doc.title,
                    "content": doc.content,
                    "content_type": doc.content_type.value,
                    "tags": doc.tags or [],
                    "kb_metadata": doc.kb_metadata or {},
                    "created_at": doc.created_at.isoformat() if doc.created_at else None,
                    "updated_at": doc.updated_at.isoformat() if doc.updated_at else None,
                    "is_processed": doc.is_processed,
                    "processing_error": doc.processing_error,
                    "knowledge_base_id": doc.knowledge_base_id,
                }
                
                # Add metadata info if available
                if metadata:
                    result["metadata_info"] = {
                        "searchable_text": metadata.searchable_text or "",
                        "keywords": metadata.keywords or [],
                        "description": metadata.description or "",
                        "doc_type": metadata.doc_type or "unknown",
                        "embedding_id": metadata.embedding_id
                    }
                
                # Add chunks info if available
                if chunks:
                    result["chunks"] = [
                        {
                            "chunk_id": chunk.id,
                            "chunk_index": chunk.chunk_index,
                            "content": chunk.content,
                            "embedding_id": chunk.embedding_id
                        }
                        for chunk in chunks
                    ]
                
                logger.info(f"âœ… Retrieved document detail: {document_id}")
                return result
        
        except Exception as e:
            logger.error(f"Failed to get document detail: {e}")
            return None
    
    async def get_relevant_context_for_workflow_generation(
        self,
        query: str,
        max_tokens: int = 30000
    ) -> str:
        """
        Get context for workflow generation
        1. Search metadata
        2. Get full content
        3. Build context
        """
        try:
            logger.info(f"ðŸ“š Getting context for workflow generation: '{query}'")
            
            # Step 1: Search metadata
            metadata_results = await self.search_metadata(
                query=query,
                limit=5
            )
            
            if not metadata_results:
                logger.warning("âš ï¸ No relevant documents found")
                return ""
            
            # Step 2: Get full content
            document_ids = [r["document_id"] for r in metadata_results]
            full_contents = await self.get_full_content(document_ids)
            
            # Step 3: Build context
            context = self._build_context_from_contents(
                full_contents,
                metadata_results,
                max_tokens
            )
            
            logger.info(f"âœ… Context built: {len(context)} chars")
            return context
        
        except Exception as e:
            logger.error(f"Failed to get relevant context: {e}")
            return ""
    
    def _build_context_from_contents(
        self,
        full_contents: List[Dict[str, Any]],
        metadata_results: List[Dict[str, Any]],
        max_tokens: int
    ) -> str:
        """Build context string from full contents"""
        context_parts = []
        current_tokens = 0
        
        # Create mapping for quick lookup
        metadata_map = {r["document_id"]: r for r in metadata_results}
        
        for content in full_contents:
            doc_id = content["document_id"]
            metadata = metadata_map.get(doc_id, {})
            
            content_tokens = len(self.tokenizer.encode(content["content"]))
            
            if current_tokens + content_tokens > max_tokens:
                logger.info(f"Reached max_tokens limit. Included {len(context_parts)} documents.")
                break
            
            similarity = metadata.get("similarity_score", "N/A")
            if isinstance(similarity, (int, float)):
                score_str = f"{similarity:.3f}"
            else:
                score_str = "N/A"
            
            context_part = f"""
**{content['title']}**
Source: {metadata.get('category', 'Unknown')}
Doc Type: {metadata.get('doc_type', 'unknown')}
Score: {score_str}

{content['content']}

---
"""
            context_parts.append(context_part)
            current_tokens += content_tokens
            logger.debug(f"Added {doc_id}: {content_tokens} tokens (total: {current_tokens}/{max_tokens})")
        
        return "\n".join(context_parts)
    
    async def get_relevant_context_for_error_fix(
        self,
        query: str,
        error_logs: Optional[str] = None,
        max_tokens: int = 30000
    ) -> str:
        """
        Get context for error fixing
        Similar to workflow generation but focused on error solutions
        """
        try:
            # Enhance query with error information
            if error_logs:
                enhanced_query = f"{query}\n\nError: {error_logs[:500]}"
            else:
                enhanced_query = query
            
            # Get context (same process)
            return await self.get_relevant_context_for_workflow_generation(
                enhanced_query,
                max_tokens
            )
        
        except Exception as e:
            logger.error(f"Failed to get error context: {e}")
            return ""
    
    async def _decompose_query_to_subqueries(
        self, 
        query: str, 
        num_queries: int = 4
    ) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìž ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ì„œë¸Œ-ì¿¼ë¦¬ë¡œ ë¶„í•´
        
        Args:
            query: ì›ë³¸ ì‚¬ìš©ìž ì¿¼ë¦¬
            num_queries: ìƒì„±í•  ì„œë¸Œ-ì¿¼ë¦¬ ê°œìˆ˜
        
        Returns:
            ì„œë¸Œ-ì¿¼ë¦¬ ëª©ë¡ ë° ë¶„í•´ ì •ë³´
        """
        try:
            decompose_prompt = f"""ë‹¹ì‹ ì€ ì •ë³´ ê²€ìƒ‰ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
        
ì‚¬ìš©ìžì˜ ì›Œí¬í”Œë¡œìš° ìš”ì²­ì„ ë¶„ì„í•˜ê³ , í•„ìš”í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìž ìš”ì²­: "{query}"

ë‹¤ìŒê³¼ ê°™ì´ {num_queries}ê°œì˜ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
1. ê° ì¿¼ë¦¬ëŠ” íŠ¹ì • ê´€ì‹¬ì‚¬ë‚˜ ê¸°ìˆ ì— ì´ˆì ì„ ë§žì¶¥ë‹ˆë‹¤
2. ì¿¼ë¦¬ë“¤ì€ ìƒí˜¸ ë³´ì™„ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤
3. ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ì˜ ì§§ì€ ë¬¸ìž¥ì´ì–´ì•¼ í•©ë‹ˆë‹¤
4. âš ï¸ ì¤‘ìš”: ì¼ë°˜ì ì¸ ë‹¨ì–´ (ì˜ˆ: "API", "ë°ì´í„°", "ì½”ë“œ")ë¥¼ ìµœëŒ€í•œ í”¼í•˜ê³  êµ¬ì²´ì ì¸ ê¸°ìˆ ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ëª…ì„ í¬í•¨í•˜ì„¸ìš”
5. ì˜ˆì‹œ: "Seleniumìœ¼ë¡œ ë™ì  ì›¹ íŽ˜ì´ì§€ í¬ë¡¤ë§" (O) vs "ì›¹ í¬ë¡¤ë§" (X)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "subqueries": [
        "ì²« ë²ˆì§¸ êµ¬ì²´ì ì¸ ì¿¼ë¦¬",
        "ë‘ ë²ˆì§¸ êµ¬ì²´ì ì¸ ì¿¼ë¦¬",
        ...
    ]
}}"""
            
            from langchain_openai import ChatOpenAI
            from langchain.schema import HumanMessage
            
            llm = ChatOpenAI(
                model=self.settings.openai_model,
                api_key=self.settings.openai_api_key,
                temperature=1,
                reasoning_effort="minimal"
            )
            
            response = await llm.ainvoke([HumanMessage(content=decompose_prompt)])
            
            # Parse JSON response
            response_text = response.content
            try:
                # Extract JSON from response
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    subqueries = result.get("subqueries", [])
                    logger.info(f"âœ… Generated {len(subqueries)} subqueries")
                    return subqueries
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse LLM response: {response_text}")
            
            # Fallback: return original query
            logger.warning("Fallback: Using original query only")
            return [query]
        
        except Exception as e:
            logger.error(f"Failed to decompose query: {e}")
            return [query]
    
    def _deduplicate_metadata_results(
        self,
        all_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
        
        Args:
            all_results: ì—¬ëŸ¬ ê²€ìƒ‰ì—ì„œ ìˆ˜ì§‘í•œ ê²°ê³¼ë“¤
        
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ (ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œ)
        """
        # ë¬¸ì„œ IDë¡œ ê·¸ë£¹í™” (ê°€ìž¥ ë†’ì€ ìœ ì‚¬ë„ë§Œ ìœ ì§€)
        seen_docs = {}
        
        for result in all_results:
            doc_id = result["document_id"]
            
            if doc_id not in seen_docs:
                seen_docs[doc_id] = result
            else:
                # ìœ ì‚¬ë„ê°€ ë” ë†’ìœ¼ë©´ ì—…ë°ì´íŠ¸
                current_score = seen_docs[doc_id].get("similarity_score", 0)
                new_score = result.get("similarity_score", 0)
                
                if isinstance(new_score, (int, float)) and isinstance(current_score, (int, float)):
                    if new_score > current_score:
                        seen_docs[doc_id] = result
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        deduped = list(seen_docs.values())
        deduped.sort(
            key=lambda x: x.get("similarity_score", 0),
            reverse=True
        )
        
        logger.info(f"ðŸ“Š Deduplicated: {len(all_results)} â†’ {len(deduped)} results")
        return deduped
    
    async def get_relevant_context_for_workflow_generation_v2(
        self,
        query: str,
        use_query_decomposition: bool = True,
        max_tokens: int = 30000
    ) -> tuple:
        """
        ê°œì„ ëœ ì›Œí¬í”Œë¡œìš° ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì¿¼ë¦¬ ë¶„í•´ í¬í•¨)
        ì„ íƒì  ì¿¼ë¦¬ ë¶„í•´ë¥¼ í†µí•´ ë” í¬ê´„ì ì¸ ê²°ê³¼ ì œê³µ
        
        Args:
            query: ì‚¬ìš©ìž ì¿¼ë¦¬
            use_query_decomposition: ì¿¼ë¦¬ ë¶„í•´ ì‚¬ìš© ì—¬ë¶€
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        
        Returns:
            (context_string, metadata_dict) íŠœí”Œ
            metadata_dictì—ëŠ” ì¿¼ë¦¬ ë¶„í•´ ê³¼ì • ì •ë³´ í¬í•¨
        """
        try:
            logger.info(f"ðŸ“š Getting context (v2) for workflow: '{query}'")
            logger.info(f"Query decomposition: {'enabled' if use_query_decomposition else 'disabled'}")
            
            all_metadata_results = []
            search_queries = [query]
            subqueries_detail = []
            
            # Step 1: ì¿¼ë¦¬ ë¶„í•´ (ì„ íƒì )
            if use_query_decomposition:
                subqueries = await self._decompose_query_to_subqueries(query, num_queries=4)
                search_queries.extend(subqueries)
                logger.info(f"ðŸ” Searching with {len(search_queries)} queries total")
            
            # Step 2: ëª¨ë“  ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            for idx, search_query in enumerate(search_queries):
                try:
                    is_original = (idx == 0)
                    query_label = "Original Query" if is_original else f"Subquery {idx}"
                    
                    logger.debug(f"Searching ({query_label}): '{search_query}'")
                    
                    # ðŸ†• ì¶”ê°€: ê° ì¿¼ë¦¬ì—ì„œ ë„ë©”ì¸ ê°ì§€ (Knowledge Base Smart Search ë°©ì‹)
                    detected_domain_obj = self.domain_service.find_domain_by_keywords(search_query)
                    domain_for_search = detected_domain_obj.name if detected_domain_obj else None
                    
                    if domain_for_search:
                        logger.info(f"  ðŸ“‚ Domain detected for ({query_label}): '{domain_for_search}'")
                    else:
                        logger.debug(f"  ðŸ“‚ No specific domain detected for ({query_label}), searching common")
                    
                    # ðŸ†• ë³€ê²½: domain íŒŒë¼ë¯¸í„° ì¶”ê°€ (Search ëŒ€ìƒ ì œí•œ)
                    metadata_results = await self.search_metadata(
                        query=search_query,
                        domain=domain_for_search,  # â† í•µì‹¬ ë³€ê²½! ë„ë©”ì¸ ì§€ì •
                        limit=3  # ê° ì¿¼ë¦¬ë‹¹ 3ê°œ (ë¶„í•´ë˜ë¯€ë¡œ ì´ 12-15ê°œ ìˆ˜ì§‘)
                    )
                    
                    all_metadata_results.extend(metadata_results)
                    
                    subqueries_detail.append({
                        "query": search_query,
                        "detected_domain": domain_for_search,  # â† ë„ë©”ì¸ ì •ë³´ ê¸°ë¡
                        "found": len(metadata_results),
                        "documents": [
                            {
                                "title": r.get("title", "Unknown"),
                                "similarity_score": r.get("similarity_score", 0),
                                "document_id": r.get("document_id"),
                                "domain": r.get("domain", "unknown")
                            }
                            for r in metadata_results
                        ]
                    })
                    
                    logger.debug(f"  Found: {len(metadata_results)} results")
                except Exception as e:
                    logger.warning(f"Search failed for '{search_query}': {e}")
            
            # Step 3: ì¤‘ë³µ ì œê±°
            if not all_metadata_results:
                logger.warning("âš ï¸ No relevant documents found")
                return "", {
                    "query_decomposed": use_query_decomposition,
                    "num_subqueries": len(search_queries) - 1,
                    "total_documents_collected": 0,
                    "unique_documents": 0,
                    "subqueries_detail": subqueries_detail
                }
            
            deduped_results = self._deduplicate_metadata_results(all_metadata_results)
            logger.info(f"âœ… Total unique documents: {len(deduped_results)}")
            
            # Step 4: ì „ì²´ ì½˜í…ì¸  ì¡°íšŒ
            document_ids = [r["document_id"] for r in deduped_results]
            full_contents = await self.get_full_content(document_ids)
            
            # Step 5: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context_from_contents(
                full_contents,
                deduped_results,
                max_tokens
            )
            
            logger.info(f"âœ… Context built: {len(context)} chars from {len(full_contents)} documents")
            
            # ë©”íƒ€ë°ì´í„° ë°˜í™˜
            metadata = {
                "query_decomposed": use_query_decomposition,
                "num_subqueries": len(search_queries) - 1,
                "total_documents_collected": len(all_metadata_results),
                "unique_documents": len(deduped_results),
                "context_length": len(context),
                "subqueries_detail": subqueries_detail,
                "original_query": query,
                "domain_detection_enabled": True  # âœ¨ ë„ë©”ì¸ ê°ì§€ í™œì„±í™” í‘œì‹œ
            }
            
            return context, metadata
        
        except Exception as e:
            logger.error(f"Failed to get relevant context (v2): {e}")
            return "", {
                "query_decomposed": use_query_decomposition,
                "num_subqueries": 0,
                "total_documents_collected": 0,
                "unique_documents": 0,
                "error": str(e),
                "subqueries_detail": []
            }
    
    async def get_relevant_context_for_workflow_with_domain_detection(
        self,
        query: str,
        max_tokens: int = 30000
    ) -> tuple:
        """
        ðŸ†• ì›Œí¬í”Œë¡œìš° ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë„ë©”ì¸ ê°ì§€ í¬í•¨)
        Knowledge Baseì˜ Smart Searchì™€ ë™ì¼í•˜ê²Œ ë™ìž‘
        
        ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ë„ë©”ì¸ ê°ì§€ í›„:
        1. ê°ì§€ëœ ë„ë©”ì¸ + common ë„ë©”ì¸ì—ì„œë§Œ ê²€ìƒ‰
        2. ê²°ê³¼ ìˆ˜ì§‘ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        
        Args:
            query: ì‚¬ìš©ìž ì¿¼ë¦¬
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        
        Returns:
            (context_string, metadata_dict) íŠœí”Œ
            metadata_dictì—ëŠ” ë„ë©”ì¸ ê°ì§€ ì •ë³´ í¬í•¨
        """
        try:
            logger.info(f"ðŸ“š Getting context for workflow (with smart domain detection): '{query}'")
            
            # Step 1: ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ë„ë©”ì¸ ê°ì§€
            detected_domain_obj = self.domain_service.find_domain_by_keywords(query)
            domain_for_search = detected_domain_obj.name if detected_domain_obj else None
            
            if domain_for_search:
                logger.info(f"ðŸ“‚ Domain detected: '{domain_for_search}'")
            else:
                logger.info(f"ðŸ“‚ No specific domain detected, searching all domains")
            
            # Step 2: ê°ì§€ëœ ë„ë©”ì¸ì—ì„œ ê²€ìƒ‰ (Smart Search ë°©ì‹)
            metadata_results = await self.search_metadata(
                query=query,
                domain=domain_for_search,  # â† í•µì‹¬! ë„ë©”ì¸ ì§€ì •
                limit=5
            )
            
            logger.info(f"âœ… Found {len(metadata_results)} results")
            
            # Step 3: ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            if not metadata_results:
                logger.warning("âš ï¸ No relevant documents found")
                return "", {
                    "query": query,
                    "detected_domain": domain_for_search,
                    "total_documents_collected": 0,
                    "unique_documents": 0,
                    "context_length": 0,
                    "domain_detection_enabled": True,
                    "method": "with_domain_detection"
                }
            
            # Step 4: ì „ì²´ ì½˜í…ì¸  ì¡°íšŒ
            document_ids = [r["document_id"] for r in metadata_results]
            full_contents = await self.get_full_content(document_ids)
            
            # Step 5: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context_from_contents(
                full_contents,
                metadata_results,
                max_tokens
            )
            
            logger.info(f"âœ… Context built: {len(context)} chars from {len(full_contents)} documents")
            
            # ë©”íƒ€ë°ì´í„° ë°˜í™˜
            metadata = {
                "query": query,
                "detected_domain": domain_for_search,
                "total_documents_collected": len(metadata_results),
                "unique_documents": len(metadata_results),
                "context_length": len(context),
                "domain_detection_enabled": True,
                "method": "with_domain_detection"
            }
            
            return context, metadata
        
        except Exception as e:
            logger.error(f"Failed to get relevant context (with domain detection): {e}", exc_info=True)
            return "", {
                "error": str(e),
                "query": query,
                "domain_detection_enabled": False,
                "method": "with_domain_detection"
            }
    
    # Backward compatibility methods for old RAG interface
    async def search_documents(
        self,
        query: str,
        category = None,
        domain: str = None,  # âœ¨ NEW: Domain filter
        limit: int = 5,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """Backward compatible search method with domain filtering"""
        logger.debug(f"Backward compat: search_documents called (domain: {domain})")
        # âœ¨ FIX: Pass all parameters correctly
        results = await self.search_metadata(query=query, domain=domain, category=category, limit=limit)
        
        # Convert to old format
        converted = []
        for r in results:
            doc = await self.get_full_content([r["document_id"]])
            if doc:
                converted.append({
                    "content": doc[0].get("content", ""),
                    "metadata": {
                        "document_id": r["document_id"],
                        "title": r["title"],
                        "category": r.get("category", "Unknown"),
                        "doc_type": r.get("doc_type", "unknown"),
                        "domain": r.get("domain", "common"),  # âœ¨ NEW: Include domain
                    },
                    "similarity_score": r["similarity_score"],
                    "distance": r.get("distance", 0)
                })
        return converted
    
    async def hybrid_search(
        self,
        query: str,
        category = None,
        domain: str = None,  # âœ¨ NEW: Domain filter
        limit: int = 5,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """Backward compatible hybrid search with domain filtering"""
        logger.debug(f"Backward compat: hybrid_search called (domain: {domain})")
        return await self.search_documents(query, category, domain, limit)
    
    def build_context(self, search_results, max_tokens: int = 30000) -> str:
        """Backward compatible context builder"""
        if not search_results:
            return ""
        
        context_parts = []
        current_tokens = 0
        
        for content in search_results:
            content_text = content.get("content", "")
            metadata = content.get("metadata", {})
            
            content_tokens = len(self.tokenizer.encode(content_text))
            
            if current_tokens + content_tokens > max_tokens:
                logger.info(f"Reached max_tokens limit. Included {len(context_parts)} documents.")
                break
            
            similarity = content.get("similarity_score", "N/A")
            if isinstance(similarity, (int, float)):
                score_str = f"{similarity:.3f}"
            else:
                score_str = "N/A"
            
            context_part = f"""
**{metadata.get('title', 'Unknown')}**
Source: {metadata.get('category', 'Unknown')}
Score: {score_str}

{content_text}

---
"""
            context_parts.append(context_part)
            current_tokens += content_tokens
        
        return "\n".join(context_parts)
    
    async def log_query(
        self,
        query_text: str,
        results_count: int,
        category: Optional[KnowledgeBaseCategory] = None,
        used_in_generation: bool = False,
        generation_success: Optional[bool] = None,
        execution_time_ms: Optional[int] = None
    ):
        """Log RAG query for analytics"""
        try:
            with get_session() as session:
                rag_query = RAGQuery(
                    query_text=query_text,
                    query_category=category,
                    results_count=results_count,
                    execution_time_ms=execution_time_ms,
                    used_in_generation=used_in_generation,
                    generation_success=generation_success
                )
                session.add(rag_query)
                session.commit()
        except Exception as e:
            logger.error(f"Failed to log query: {e}")
    
    async def smart_search(
        self,
        query: str,
        limit: int = 5,
        min_score: float = 0.3
    ) -> Dict[str, Any]:
        """
        ðŸŽ¯ Smart search: Automatically detect domain and search
        
        Workflow:
        1. Detect domain from query using DomainService
        2. Search in detected domain collection (if found)
        3. Always search in common domain
        4. Merge results and remove duplicates
        5. Sort by similarity score
        
        Args:
            query: User query (natural language)
            limit: Maximum results per domain
            min_score: Minimum similarity score (0-1)
        
        Returns:
            {
                "detected_domain": "ë„¤ì´ë²„" or None,
                "domain_results": [...],  # Results from specific domain
                "common_results": [...],  # Results from common domain
                "all_results": [...],     # Merged results (sorted by score)
                "total_count": int
            }
        """
        logger.info(f"ðŸ” Smart search: '{query}'")
        
        domain_results = []
        common_results = []
        detected_domain = None
        
        # Step 1: Detect domain from query
        detected_domain_obj = self.domain_service.find_domain_by_keywords(query)
        
        if detected_domain_obj:
            detected_domain = detected_domain_obj.name
            logger.info(f"ðŸ“‚ Detected domain: '{detected_domain}'")
            
            # Step 2: Search in specific domain
            try:
                collection = self._get_collection_by_name(detected_domain_obj.collection_name)
                
                results = collection.query(
                    query_texts=[query],
                    n_results=limit,
                    include=["documents", "metadatas", "distances"]
                )
                
                domain_results = self._parse_search_results(results, min_score=min_score)
                logger.info(f"  âœ… Found {len(domain_results)} results in '{detected_domain}'")
                
            except Exception as e:
                logger.error(f"  âŒ Domain search failed: {e}")
        else:
            logger.info(f"ðŸ“‚ No specific domain detected, searching common only")
        
        # Step 3: Always search in common domain
        try:
            common_domain = self.domain_service.get_common_domain()
            
            if common_domain:
                collection = self._get_collection_by_name(common_domain.collection_name)
                
                results = collection.query(
                    query_texts=[query],
                    n_results=limit,
                    include=["documents", "metadatas", "distances"]
                )
                
                common_results = self._parse_search_results(results, min_score=min_score)
                logger.info(f"  âœ… Found {len(common_results)} results in 'common'")
            
        except Exception as e:
            logger.error(f"  âŒ Common search failed: {e}")
        
        # Step 4: Merge results and remove duplicates
        all_results = []
        seen_ids = set()
        
        # Add domain-specific results first (higher priority)
        for result in domain_results:
            doc_id = result.get("document_id")
            if doc_id and doc_id not in seen_ids:
                result["source_domain"] = detected_domain
                all_results.append(result)
                seen_ids.add(doc_id)
        
        # Add common results
        for result in common_results:
            doc_id = result.get("document_id")
            if doc_id and doc_id not in seen_ids:
                result["source_domain"] = "common"
                all_results.append(result)
                seen_ids.add(doc_id)
        
        # Step 5: Sort by similarity score (descending)
        all_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
        
        # Limit total results
        all_results = all_results[:limit]
        
        logger.info(f"âœ… Smart search complete: {len(all_results)} total results")
        
        return {
            "detected_domain": detected_domain,
            "domain_results": domain_results,
            "common_results": common_results,
            "all_results": all_results,
            "total_count": len(all_results)
        }
    
    def _get_collection_by_name(self, collection_name: str):
        """
        Get ChromaDB collection by name
        
        Args:
            collection_name: Collection name (e.g., "collection_ë„¤ì´ë²„")
        
        Returns:
            ChromaDB Collection object
        """
        cache_key = f"name_{collection_name}"
        
        if cache_key not in self._collections_cache:
            try:
                # Try to get existing collection
                collection = self.chroma_client.get_collection(
                    name=collection_name,
                    embedding_function=self.embedding_function
                )
                self._collections_cache[cache_key] = collection
                logger.debug(f"ðŸ“‚ Loaded collection: {collection_name}")
                
            except Exception as e:
                # Collection doesn't exist, create it
                logger.info(f"âœ¨ Creating new collection: {collection_name}")
                collection = self.chroma_client.create_collection(
                    name=collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"hnsw:space": "cosine"}
                )
                self._collections_cache[cache_key] = collection
        
        return self._collections_cache[cache_key]
    
    def _parse_search_results(
        self,
        results: Dict[str, Any],
        min_score: float = 0.0
    ) -> List[Dict[str, Any]]:
        """
        Parse ChromaDB search results
        
        Args:
            results: ChromaDB query results
            min_score: Minimum similarity score filter
        
        Returns:
            List of parsed result dictionaries
        """
        parsed_results = []
        
        if not results or not results.get("ids"):
            return parsed_results
        
        ids = results["ids"][0] if results["ids"] else []
        metadatas = results["metadatas"][0] if results.get("metadatas") else []
        distances = results["distances"][0] if results.get("distances") else []
        documents = results["documents"][0] if results.get("documents") else []
        
        for i, doc_id in enumerate(ids):
            metadata = metadatas[i] if i < len(metadatas) else {}
            distance = distances[i] if i < len(distances) else 1.0
            document = documents[i] if i < len(documents) else ""
            
            # Convert distance to similarity score (cosine: 0=identical, 2=opposite)
            similarity_score = 1.0 - (distance / 2.0)
            
            # Filter by minimum score
            if similarity_score < min_score:
                continue
            
            parsed_results.append({
                "document_id": metadata.get("document_id", doc_id),
                "title": metadata.get("title", "Untitled"),
                "domain": metadata.get("domain", "unknown"),
                "doc_type": metadata.get("doc_type", "unknown"),
                "content_type": metadata.get("content_type", "unknown"),
                "similarity_score": similarity_score,
                "distance": distance,
                "searchable_text": document[:200] + "..." if len(document) > 200 else document
            })
        
        return parsed_results


# Global RAG service instance
_rag_service_instance = None


def get_rag_service() -> RAGService:
    """Get or create RAG service singleton"""
    global _rag_service_instance
    if _rag_service_instance is None:
        _rag_service_instance = RAGService()
    return _rag_service_instance
